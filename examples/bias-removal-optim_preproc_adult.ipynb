{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "demo_optim_preproc_adult.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnanmasood/AIF360/blob/master/examples/bias-removal-optim_preproc_adult.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv0qVACVj5LR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "10dccac6-9409-4e58-95be-b082e581684d"
      },
      "source": [
        "!pip install git+https://git@github.com/adnanmasood/AIF360.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/bhomass/AIF360.git\n",
            "  Cloning https://****@github.com/bhomass/AIF360.git to /tmp/pip-req-build-igt74688\n",
            "  Running command git clone -q 'https://****@github.com/bhomass/AIF360.git' /tmp/pip-req-build-igt74688\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from aif360==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from aif360==0.3.0) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from aif360==0.3.0) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.6/dist-packages (from aif360==0.3.0) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from aif360==0.3.0) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360==0.3.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->aif360==0.3.0) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21->aif360==0.3.0) (0.15.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360==0.3.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360==0.3.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->aif360==0.3.0) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24.0->aif360==0.3.0) (1.12.0)\n",
            "Building wheels for collected packages: aif360\n",
            "  Building wheel for aif360 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aif360: filename=aif360-0.3.0-cp36-none-any.whl size=1226976 sha256=f7187afcfb557d8511fb8ddc5d9ebdac8864f1817f09783b9100c61f9beb1468\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-utd7e75k/wheels/b0/cb/cf/e61262f367d6e3f0d65ae1e5cb7809b5d9952f1c0e64eaae42\n",
            "Successfully built aif360\n",
            "Installing collected packages: aif360\n",
            "Successfully installed aif360-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzEqsGTjjsYK",
        "colab_type": "text"
      },
      "source": [
        "# Detecting and mitigating racial bias in income estimation \n",
        "\n",
        "it needs to know the attribute or attributes, called _protected attributes_, that are of interest: race is one example of a _protected attribute_ and age is a second.\n",
        "\n",
        "First, the process starts with a _training dataset_, which contains a sequence of instances, where each instance has two components: the features and the correct prediction for those features.  Next, a machine learning algorithm is trained on this training dataset to produce a machine learning model.  This generated model can be used to make a prediction when given a new instance.  A second dataset with features and correct predictions, called a _test dataset_, is used to assess the accuracy of the model.\n",
        "\n",
        "Since this test dataset is the same format as the training dataset, a set of instances of features and prediction pairs, often these two datasets derive from the same initial dataset.  A random partitioning algorithm is used to split the initial dataset into training and test datasets.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpzbVyMljsYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from aif360.metrics import BinaryLabelDatasetMetric\n",
        "\n",
        "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
        "            import load_preproc_data_adult\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions\\\n",
        "            import get_distortion_adult\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "\n",
        "from IPython.display import Markdown, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDsGEElajsYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-swo9XwjsYS",
        "colab_type": "text"
      },
      "source": [
        "### Step 2 Load dataset, specifying protected attribute, and split dataset into train and test\n",
        "In Step 2 we load the initial dataset, setting the protected attribute to be race.  We then splits the original dataset into training and testing datasets.  A normal workflow would also use a test dataset for assessing the efficacy (accuracy, fairness, etc.) during the development of a machine learning model.  Finally, we set two variables (to be used in Step 3) for the privileged (1) and unprivileged (0) values for the race attribute.  These are key inputs for detecting and mitigating bias, which will be Step 3 and Step 4.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJkekKNejsYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_orig = load_preproc_data_adult(['race'])\n",
        "\n",
        "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
        "\n",
        "privileged_groups = [{'race': 1}] # White\n",
        "unprivileged_groups = [{'race': 0}] # Not white"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j99hKoppjsYV",
        "colab_type": "text"
      },
      "source": [
        "### Step 3 Compute fairness metric on original training dataset\n",
        "Now that we've identified the protected attribute 'race' and defined privileged and unprivileged values, we can detect bias in the dataset.  One simple test is to compare the percentage of favorable results for the privileged and unprivileged groups, subtracting the former percentage from the latter.   A negative value indicates less favorable outcomes for the unprivileged groups.  This is implemented in the method called mean_difference on the BinaryLabelDatasetMetric class.  The code below performs this check and displays the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9iQrEvnjsYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "84de808c-0121-4bd9-8b42-c2d056ebc186"
      },
      "source": [
        "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
        "                                             unprivileged_groups=unprivileged_groups,\n",
        "                                             privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Original training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Original training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = -0.097036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr7n5cVujsYb",
        "colab_type": "text"
      },
      "source": [
        "### Step 4 Mitigate bias by transforming the original dataset\n",
        "The previous step showed that the privileged group was getting 10.5% more positive outcomes in the training dataset.   Since this is not desirable, we are going to try to mitigate this bias in the training dataset. Optimized preprocessing algorithm will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups.\n",
        "\n",
        "The algorithm requires some tuning parameters, which are set in the optim_options variable and passed as an argument along with some other parameters, including the 2 variables containg the unprivileged and privileged groups defined in Step 3.\n",
        "\n",
        "We then call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (dataset_transf_train).  Finally, we ensure alignment of features between the transformed and the original dataset to enable comparisons.\n",
        "\n",
        "[1] Optimized Pre-Processing for Discrimination Prevention, NIPS 2017, Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R. Varshney"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE31_csJjsYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "87123b4d-abd9-4d63-8770-377e71843b5d"
      },
      "source": [
        "optim_options = {\n",
        "    \"distortion_fun\": get_distortion_adult,\n",
        "    \"epsilon\": 0.05,\n",
        "    \"clist\": [0.99, 1.99, 2.99],\n",
        "    \"dlist\": [.1, 0.05, 0]\n",
        "}\n",
        "    \n",
        "OP = OptimPreproc(OptTools, optim_options)\n",
        "\n",
        "OP = OP.fit(dataset_orig_train)\n",
        "dataset_transf_train = OP.transform(dataset_orig_train, transform_Y=True)\n",
        "\n",
        "dataset_transf_train = dataset_orig_train.align_datasets(dataset_transf_train)\n",
        "\n",
        "print(dataset_transf_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimized Preprocessing: Objective converged to 0.000000\n",
            "               instance weights            features  ...                     labels\n",
            "                                protected attribute  ...                           \n",
            "                                               race  ... Education Years=>12       \n",
            "instance names                                       ...                           \n",
            "391                         1.0                 0.0  ...                 0.0    0.0\n",
            "1899                        1.0                 0.0  ...                 0.0    0.0\n",
            "24506                       1.0                 1.0  ...                 1.0    1.0\n",
            "32816                       1.0                 1.0  ...                 1.0    0.0\n",
            "47892                       1.0                 1.0  ...                 1.0    0.0\n",
            "...                         ...                 ...  ...                 ...    ...\n",
            "4449                        1.0                 1.0  ...                 1.0    0.0\n",
            "39024                       1.0                 1.0  ...                 0.0    0.0\n",
            "29785                       1.0                 0.0  ...                 0.0    0.0\n",
            "37368                       1.0                 1.0  ...                 1.0    0.0\n",
            "43298                       1.0                 1.0  ...                 0.0    0.0\n",
            "\n",
            "[34189 rows x 20 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivhcN8KRmoBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To export the dataset\n",
        "#pd.set_option('display.max_rows', 50)\n",
        "#from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "#drive.mount('/gdrive')\n",
        "#data = pd.DataFrame()\n",
        "#data, _ = dataset_transf_train.convert_to_dataframe()\n",
        "# Write the DataFrame to CSV file.\n",
        "#with open('/gdrive/My Drive/output/dataset.csv', 'w') as f:\n",
        "#  data.to_csv(f)\n",
        "#print (\"completed\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ev-7EzGjsYf",
        "colab_type": "text"
      },
      "source": [
        "### Step 5 Compute fairness metric on transformed dataset\n",
        "Now that we have a transformed dataset, we can check how effective it was in removing bias by using the same metric we used for the original training dataset in Step 3.  Once again, we use the function mean_difference in the BinaryLabelDatasetMetric class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSnmkv8ejsYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "88d51365-29a1-4081-f44e-a52437b9ed84"
      },
      "source": [
        "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
        "                                               unprivileged_groups=unprivileged_groups,\n",
        "                                               privileged_groups=privileged_groups)\n",
        "display(Markdown(\"#### Transformed training dataset\"))\n",
        "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Transformed training dataset",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Difference in mean outcomes between unprivileged and privileged groups = -0.050695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEeRapD9jsYi",
        "colab_type": "text"
      },
      "source": [
        "We see the mitigation step was very effective, the difference in mean outcomes is now -0.051074.  So we went from a 10.5% advantage for the privileged group to a 5.1% advantage for the privileged group &mdash; a reduction in more than half!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da9o6e4pjsYi",
        "colab_type": "text"
      },
      "source": [
        "### Summary\n",
        "\n",
        "As mentioned earlier, both fairness metrics and mitigation algorithms can be performed at various stages of the machine learning pipeline.  We recommend checking for bias as often as possible, using as many metrics are relevant for the application domain.  We also recommend incorporating bias detection in an automated continous integration pipeline to ensure bias awareness as a software project evolves."
      ]
    }
  ]
}